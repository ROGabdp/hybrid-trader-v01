# -*- coding: utf-8 -*-
"""
================================================================================
Daily Operations with Dual Strategy & Versioning (v2 - Fixed)
================================================================================
æ¯æ—¥ç¶­é‹è…³æœ¬ - é›™ç­–ç•¥æ¨è«–èˆ‡ç‰ˆæœ¬æ§ç®¡

ä¿®æ­£é‡é»ï¼š
1. å¼•ç”¨ä¸»ç³»çµ± (ptrl_hybrid_system) ç¢ºä¿ç‰¹å¾µå·¥ç¨‹ä¸€è‡´æ€§
2. é€éæ¨¡å‹æ³¨å…¥ (Model Injection) å¼·åˆ¶ä½¿ç”¨ç•¶æ—¥è¨“ç·´çš„ LSTM æ¨¡å‹
3. ä½¿ç”¨ subprocess åŸ·è¡Œ LSTM è¨“ç·´ä»¥é‡‹æ”¾ GPU è¨˜æ†¶é«”

åŠŸèƒ½ï¼š
1. å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€ (daily_runs/{date}/)
2. LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜ (subprocess)
3. éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥ + ä¸»ç³»çµ±è¨ˆç®—)
4. é›™æ¨¡å‹æ¨è«– (Aggressive vs Conservative)
5. è¼¸å‡ºæˆ°æƒ…å„€è¡¨æ¿èˆ‡æ—¥èªŒ

ä½œè€…ï¼šPhil Liang
æ—¥æœŸï¼š2025-12-07
================================================================================
"""

import os
import sys
import shutil
import pickle
import subprocess
import json
import glob
from datetime import datetime, timedelta

# è¨­å®š UTF-8 è¼¸å‡º
sys.stdout.reconfigure(encoding='utf-8')

# æŠ‘åˆ¶ TensorFlow è­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import yfinance as yf

# =============================================================================
# å¼•ç”¨ä¸»ç³»çµ± (é—œéµä¿®æ­£)
# =============================================================================
import ptrl_hybrid_system as core_system

# =============================================================================
# è¨­å®šè·¯å¾‘
# =============================================================================
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
DAILY_RUNS_PATH = os.path.join(PROJECT_PATH, 'daily_runs')

# RL æ¨¡å‹è·¯å¾‘ (å›ºå®š)
STRATEGY_A_PATH = os.path.join(PROJECT_PATH, 'models_hybrid')  # Aggressive
STRATEGY_B_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v2_conservative')  # Conservative

# LSTM æ¨¡å‹é è¨­è·¯å¾‘
DEFAULT_LSTM_5D_PATH = os.path.join(PROJECT_PATH, 'saved_models_5d')
DEFAULT_LSTM_1D_PATH = os.path.join(PROJECT_PATH, 'saved_models_multivariate')


# =============================================================================
# Step 0: å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€
# =============================================================================
def create_daily_workspace(date_str: str) -> dict:
    """å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€ç›®éŒ„çµæ§‹"""
    
    daily_path = os.path.join(DAILY_RUNS_PATH, date_str)
    
    paths = {
        'root': daily_path,
        'lstm_models': os.path.join(daily_path, 'lstm_models'),
        'lstm_5d': os.path.join(daily_path, 'lstm_models', 'saved_models_5d'),
        'lstm_1d': os.path.join(daily_path, 'lstm_models', 'saved_models_multivariate'),
        'cache': os.path.join(daily_path, 'cache'),
        'reports': os.path.join(daily_path, 'reports'),
    }
    
    for key, path in paths.items():
        os.makedirs(path, exist_ok=True)
    
    print(f"[Workspace] å»ºç«‹ç•¶æ—¥å·¥ä½œå€: {daily_path}")
    return paths


# =============================================================================
# Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜ (ä½¿ç”¨ subprocess)
# =============================================================================
def train_and_archive_lstm(workspace: dict, end_date: str):
    """
    ä½¿ç”¨ subprocess è¨“ç·´ LSTM æ¨¡å‹ä¸¦å°å­˜åˆ°ç•¶æ—¥å·¥ä½œå€
    
    ä½¿ç”¨ subprocess çš„å¥½è™•ï¼š
    - è¨“ç·´çµæŸå¾Œè‡ªå‹•é‡‹æ”¾ GPU è¨˜æ†¶é«”
    - é¿å…è¨“ç·´éç¨‹ä¸­çš„è¨˜æ†¶é«”æ´©æ¼å½±éŸ¿å¾ŒçºŒæ¨è«–
    
    Args:
        workspace: ç•¶æ—¥å·¥ä½œå€è·¯å¾‘å­—å…¸
        end_date: è¨“ç·´çµæŸæ—¥æœŸ (YYYY-MM-DD)
    """
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜ (subprocess)")
    print("=" * 60)
    
    # =========================================================================
    # ä½¿ç”¨ subprocess åŸ·è¡Œè¨“ç·´è…³æœ¬
    # =========================================================================
    train_script = os.path.join(PROJECT_PATH, 'train_lstm_models.py')
    
    if os.path.exists(train_script):
        print(f"\n[Training] åŸ·è¡Œ LSTM è¨“ç·´è…³æœ¬...")
        print(f"[Training] çµæŸæ—¥æœŸ: {end_date}")
        
        try:
            # åŸ·è¡Œè¨“ç·´è…³æœ¬ (åœ¨ç¨ç«‹é€²ç¨‹ä¸­)
            result = subprocess.run(
                [sys.executable, train_script],
                cwd=PROJECT_PATH,
                capture_output=True,
                text=True,
                timeout=600  # 10 åˆ†é˜è¶…æ™‚
            )
            
            if result.returncode == 0:
                print("[Training] âœ… LSTM è¨“ç·´å®Œæˆ")
            else:
                print(f"[Training] âš ï¸ è¨“ç·´è…³æœ¬è¿”å›éé›¶ä»£ç¢¼: {result.returncode}")
                if result.stderr:
                    print(f"[Training] stderr: {result.stderr[:500]}")
                    
        except subprocess.TimeoutExpired:
            print("[Training] âš ï¸ è¨“ç·´è¶…æ™‚ (10 åˆ†é˜)")
        except Exception as e:
            print(f"[Training] âš ï¸ åŸ·è¡Œè¨“ç·´è…³æœ¬å¤±æ•—: {e}")
    else:
        print(f"[Warning] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {train_script}")
        print("[Warning] å°‡ä½¿ç”¨ç¾æœ‰æ¨¡å‹...")
    
    # =========================================================================
    # å°å­˜æ¨¡å‹åˆ°ç•¶æ—¥å·¥ä½œå€
    # =========================================================================
    print("\n[Archive] å°å­˜æ¨¡å‹åˆ°ç•¶æ—¥å·¥ä½œå€...")
    
    # è¤‡è£½ T+5 æ¨¡å‹
    if os.path.exists(DEFAULT_LSTM_5D_PATH):
        dest_dir = workspace['lstm_5d']
        
        # è¤‡è£½æ‰€æœ‰æ¨¡å‹æª”æ¡ˆ
        for file_pattern in ['*.keras', '*.pkl', '*.json', '*.png']:
            for src_file in glob.glob(os.path.join(DEFAULT_LSTM_5D_PATH, file_pattern)):
                dest_file = os.path.join(dest_dir, os.path.basename(src_file))
                shutil.copy2(src_file, dest_file)
        
        print(f"  âœ… T+5 æ¨¡å‹å·²å°å­˜: {dest_dir}")
    else:
        print(f"  âš ï¸ æ‰¾ä¸åˆ° T+5 æ¨¡å‹: {DEFAULT_LSTM_5D_PATH}")
    
    # è¤‡è£½ T+1 æ¨¡å‹
    if os.path.exists(DEFAULT_LSTM_1D_PATH):
        dest_dir = workspace['lstm_1d']
        
        for file_pattern in ['*.keras', '*.pkl', '*.json', '*.png']:
            for src_file in glob.glob(os.path.join(DEFAULT_LSTM_1D_PATH, file_pattern)):
                dest_file = os.path.join(dest_dir, os.path.basename(src_file))
                shutil.copy2(src_file, dest_file)
        
        print(f"  âœ… T+1 æ¨¡å‹å·²å°å­˜: {dest_dir}")
    else:
        print(f"  âš ï¸ æ‰¾ä¸åˆ° T+1 æ¨¡å‹: {DEFAULT_LSTM_1D_PATH}")
    
    return True


# =============================================================================
# Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥ + ä¸»ç³»çµ±è¨ˆç®—)
# =============================================================================
def isolated_feature_engineering(workspace: dict, end_date: str) -> pd.DataFrame:
    """
    ä½¿ç”¨ç•¶æ—¥å°å­˜çš„ LSTM æ¨¡å‹é€²è¡Œç‰¹å¾µå·¥ç¨‹
    
    é—œéµä¿®æ­£ï¼š
    1. å¾ç•¶æ—¥å·¥ä½œå€è¼‰å…¥ LSTM æ¨¡å‹
    2. é€éæ¨¡å‹æ³¨å…¥ (Monkey Patching) è¦†è“‹ core_system._LSTM_MODELS
    3. å‘¼å« core_system.calculate_features() ç¢ºä¿ç‰¹å¾µè¨ˆç®—ä¸€è‡´
    
    Args:
        workspace: ç•¶æ—¥å·¥ä½œå€è·¯å¾‘å­—å…¸
        end_date: æ•¸æ“šçµæŸæ—¥æœŸ
    
    Returns:
        åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„ DataFrame
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)")
    print("=" * 60)
    
    import tensorflow as tf
    from tensorflow import keras
    from keras import layers
    
    # =========================================================================
    # å®šç¾© SelfAttention å±¤ (èˆ‡è¨“ç·´æ™‚ç›¸åŒ)
    # =========================================================================
    class SelfAttention(layers.Layer):
        def __init__(self, **kwargs):
            super(SelfAttention, self).__init__(**kwargs)
        
        def build(self, input_shape):
            self.units = input_shape[-1]
            self.W_q = self.add_weight(name='W_query', shape=(self.units, self.units),
                                       initializer='glorot_uniform', trainable=True)
            self.W_k = self.add_weight(name='W_key', shape=(self.units, self.units),
                                       initializer='glorot_uniform', trainable=True)
        
        def call(self, inputs, training=None):
            Q = tf.matmul(inputs, self.W_q)
            K = tf.matmul(inputs, self.W_k)
            attention = tf.nn.softmax(tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.units, tf.float32)))
            return tf.matmul(attention, inputs)
    
    # =========================================================================
    # å¾ç•¶æ—¥å·¥ä½œå€è¼‰å…¥ LSTM æ¨¡å‹
    # =========================================================================
    print("\n[Model Injection] è¼‰å…¥ç•¶æ—¥å°å­˜çš„ LSTM æ¨¡å‹...")
    
    model_5d, scaler_5d, meta_5d = None, None, None
    model_1d, scaler_1d, meta_1d = None, None, None
    
    # è¼‰å…¥ T+5 æ¨¡å‹
    lstm_5d_path = workspace['lstm_5d']
    keras_files_5d = glob.glob(os.path.join(lstm_5d_path, "*.keras"))
    
    if keras_files_5d:
        latest_keras = sorted(keras_files_5d)[-1]
        model_5d = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})
        
        # è¼‰å…¥ scaler
        scaler_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
        if os.path.exists(scaler_file):
            with open(scaler_file, 'rb') as f:
                scaler_5d = pickle.load(f)
        
        # è¼‰å…¥ meta
        meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
        if os.path.exists(meta_file):
            with open(meta_file, 'r') as f:
                meta_5d = json.load(f)
        
        print(f"  âœ… T+5 æ¨¡å‹: {os.path.basename(latest_keras)}")
    else:
        print(f"  âš ï¸ æ‰¾ä¸åˆ° T+5 æ¨¡å‹æª”æ¡ˆ: {lstm_5d_path}")
    
    # è¼‰å…¥ T+1 æ¨¡å‹
    lstm_1d_path = workspace['lstm_1d']
    keras_files_1d = glob.glob(os.path.join(lstm_1d_path, "*.keras"))
    
    if keras_files_1d:
        latest_keras = sorted(keras_files_1d)[-1]
        model_1d = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})
        
        # è¼‰å…¥ scaler
        scaler_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
        if os.path.exists(scaler_file):
            with open(scaler_file, 'rb') as f:
                scaler_1d = pickle.load(f)
        
        # è¼‰å…¥ meta
        meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
        if os.path.exists(meta_file):
            with open(meta_file, 'r') as f:
                meta_1d = json.load(f)
        
        print(f"  âœ… T+1 æ¨¡å‹: {os.path.basename(latest_keras)}")
    else:
        print(f"  âš ï¸ æ‰¾ä¸åˆ° T+1 æ¨¡å‹æª”æ¡ˆ: {lstm_1d_path}")
    
    # =========================================================================
    # æ¨¡å‹æ³¨å…¥ (Monkey Patching core_system._LSTM_MODELS)
    # =========================================================================
    print("\n[Model Injection] æ³¨å…¥æ¨¡å‹åˆ°ä¸»ç³»çµ±...")
    
    # ç¢ºä¿ _LSTM_MODELS å­—å…¸å­˜åœ¨
    if not hasattr(core_system, '_LSTM_MODELS'):
        core_system._LSTM_MODELS = {}
    
    # æ³¨å…¥ T+5 æ¨¡å‹
    core_system._LSTM_MODELS['model_5d'] = model_5d
    core_system._LSTM_MODELS['scaler_feat_5d'] = scaler_5d
    core_system._LSTM_MODELS['meta_5d'] = meta_5d
    
    # æ³¨å…¥ T+1 æ¨¡å‹
    core_system._LSTM_MODELS['model_1d'] = model_1d
    core_system._LSTM_MODELS['scaler_feat_1d'] = scaler_1d
    core_system._LSTM_MODELS['meta_1d'] = meta_1d
    
    # æ¨™è¨˜ç‚ºå·²è¼‰å…¥
    core_system._LSTM_MODELS['loaded'] = True
    
    print("  âœ… æ¨¡å‹æ³¨å…¥å®Œæˆ")
    
    # =========================================================================
    # ä¸‹è¼‰æœ€æ–°æ•¸æ“š
    # =========================================================================
    print("\n[Data] ä¸‹è¼‰ ^TWII æ•¸æ“š...")
    
    # ä¸‹è¼‰è¶³å¤ é•·çš„æ­·å²æ•¸æ“šä»¥è¨ˆç®—æ‰€æœ‰æŒ‡æ¨™
    raw_df = yf.download("^TWII", start="2020-01-01", end=end_date, auto_adjust=True, progress=False)
    
    if isinstance(raw_df.columns, pd.MultiIndex):
        raw_df.columns = raw_df.columns.get_level_values(0)
    
    print(f"[Data] æ•¸æ“šç¯„åœ: {raw_df.index[0].strftime('%Y-%m-%d')} ~ {raw_df.index[-1].strftime('%Y-%m-%d')}")
    print(f"[Data] ç¸½ç­†æ•¸: {len(raw_df)}")
    
    # =========================================================================
    # ä½¿ç”¨ä¸»ç³»çµ±è¨ˆç®—ç‰¹å¾µ (é—œéµä¿®æ­£)
    # =========================================================================
    print("\n[Features] ä½¿ç”¨ä¸»ç³»çµ±è¨ˆç®—ç‰¹å¾µ (ç¢ºä¿ä¸€è‡´æ€§)...")
    
    # å‘¼å«ä¸»ç³»çµ±çš„ calculate_features å‡½æ•¸
    # é€™ç¢ºä¿æ‰€æœ‰æŒ‡æ¨™è¨ˆç®—é‚è¼¯èˆ‡è¨“ç·´æ™‚ 100% ä¸€è‡´
    try:
        df = core_system.calculate_features(
            df=raw_df.copy(),
            benchmark_df=raw_df.copy(),  # ä½¿ç”¨è‡ªèº«ä½œç‚º benchmark
            ticker="^TWII",
            use_cache=False  # ä¸ä½¿ç”¨å¿«å–ï¼Œç¢ºä¿é‡æ–°è¨ˆç®—
        )
        print(f"[Features] âœ… ç‰¹å¾µè¨ˆç®—å®Œæˆï¼Œç¸½æ¬„ä½æ•¸: {len(df.columns)}")
    except Exception as e:
        print(f"[Features] âš ï¸ ç‰¹å¾µè¨ˆç®—å¤±æ•—: {e}")
        print("[Features] å˜—è©¦ä½¿ç”¨ç°¡åŒ–ç‰¹å¾µ...")
        df = raw_df.copy()
    
    # =========================================================================
    # å„²å­˜å¿«å–åˆ°ç•¶æ—¥å·¥ä½œå€
    # =========================================================================
    cache_path = os.path.join(workspace['cache'], 'twii_features.pkl')
    with open(cache_path, 'wb') as f:
        pickle.dump(df, f)
    
    print(f"\n[Cache] ç‰¹å¾µå·²å„²å­˜: {cache_path}")
    print(f"[Cache] æœ€çµ‚ç­†æ•¸: {len(df)}")
    
    return df


# =============================================================================
# Step 3: é›™æ¨¡å‹æ¨è«–
# =============================================================================
def dual_inference(workspace: dict, df: pd.DataFrame) -> dict:
    """
    ä½¿ç”¨å…©å¥— RL æ¨¡å‹é€²è¡Œæ¨è«–
    
    Args:
        workspace: ç•¶æ—¥å·¥ä½œå€è·¯å¾‘å­—å…¸
        df: ç‰¹å¾µ DataFrame
    
    Returns:
        åŒ…å«å…©å¥—ç­–ç•¥å»ºè­°çš„å­—å…¸
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ Step 3: é›™æ¨¡å‹æ¨è«–")
    print("=" * 60)
    
    from stable_baselines3 import PPO
    
    # ä½¿ç”¨ä¸»ç³»çµ±å®šç¾©çš„ç‰¹å¾µæ¬„ä½ (ç¢ºä¿ä¸€è‡´æ€§)
    FEATURE_COLS = core_system.FEATURE_COLS
    
    # å–å¾—æœ€æ–°ä¸€ç­†æ•¸æ“š
    latest = df.iloc[-1]
    
    # æº–å‚™ç‰¹å¾µå‘é‡
    available_cols = [c for c in FEATURE_COLS if c in df.columns]
    
    if len(available_cols) < len(FEATURE_COLS):
        missing = set(FEATURE_COLS) - set(available_cols)
        print(f"[Warning] ç¼ºå°‘ç‰¹å¾µæ¬„ä½: {missing}")
    
    features = latest[available_cols].values.astype(np.float32)
    
    # è£œé½Šç¼ºå¤±çš„æ¬„ä½ (å¡«å…… 0)
    if len(features) < len(FEATURE_COLS):
        features = np.pad(features, (0, len(FEATURE_COLS) - len(features)), mode='constant', constant_values=0)
    
    # è™•ç† NaN
    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
    
    results = {}
    
    # =========================================================================
    # Strategy A: Aggressive (ROI 85%)
    # =========================================================================
    print("\n[Strategy A] Aggressive (ROI 85%)...")
    
    buy_a_path = os.path.join(STRATEGY_A_PATH, 'ppo_buy_twii_final.zip')
    sell_a_path = os.path.join(STRATEGY_A_PATH, 'ppo_sell_twii_final.zip')
    
    if os.path.exists(buy_a_path) and os.path.exists(sell_a_path):
        try:
            buy_model_a = PPO.load(buy_a_path)
            sell_model_a = PPO.load(sell_a_path)
            
            # Buy æ¨è«–
            buy_action_a, _ = buy_model_a.predict(features, deterministic=True)
            
            # è¨ˆç®—ä¿¡å¿ƒåº¦
            try:
                obs_tensor = buy_model_a.policy.obs_to_tensor(features.reshape(1, -1))[0]
                buy_probs_a = buy_model_a.policy.get_distribution(obs_tensor).distribution.probs.detach().numpy()[0]
                buy_confidence = float(buy_probs_a[1]) if buy_action_a == 1 else float(buy_probs_a[0])
            except:
                buy_confidence = 0.5
            
            # Sell æ¨è«– (éœ€è¦åŠ å…¥æŒæœ‰å ±é…¬)
            sell_features = np.concatenate([features, [1.0]])  # å‡è¨­æŒæœ‰å ±é…¬ 0%
            sell_action_a, _ = sell_model_a.predict(sell_features, deterministic=True)
            
            results['strategy_a'] = {
                'name': 'Aggressive (ROI 85%)',
                'buy_action': int(buy_action_a),
                'buy_signal': 'BUY' if buy_action_a == 1 else 'HOLD',
                'buy_confidence': buy_confidence,
                'sell_action': int(sell_action_a),
                'sell_signal': 'SELL' if sell_action_a == 1 else 'HOLD',
            }
            print(f"  Buy: {results['strategy_a']['buy_signal']} (Conf: {results['strategy_a']['buy_confidence']:.2%})")
            print(f"  Sell: {results['strategy_a']['sell_signal']}")
            
        except Exception as e:
            print(f"  [Error] æ¨è«–å¤±æ•—: {e}")
            results['strategy_a'] = {'name': 'Aggressive', 'error': str(e)}
    else:
        print(f"  [Warning] æ‰¾ä¸åˆ°æ¨¡å‹")
        results['strategy_a'] = {'name': 'Aggressive', 'error': 'Model not found'}
    
    # =========================================================================
    # Strategy B: Conservative (MDD -6%)
    # =========================================================================
    print("\n[Strategy B] Conservative (MDD -6%)...")
    
    buy_b_path = os.path.join(STRATEGY_B_PATH, 'ppo_buy_twii_final.zip')
    sell_b_path = os.path.join(STRATEGY_B_PATH, 'ppo_sell_twii_final.zip')
    
    if os.path.exists(buy_b_path) and os.path.exists(sell_b_path):
        try:
            buy_model_b = PPO.load(buy_b_path)
            sell_model_b = PPO.load(sell_b_path)
            
            # Buy æ¨è«–
            buy_action_b, _ = buy_model_b.predict(features, deterministic=True)
            
            try:
                obs_tensor = buy_model_b.policy.obs_to_tensor(features.reshape(1, -1))[0]
                buy_probs_b = buy_model_b.policy.get_distribution(obs_tensor).distribution.probs.detach().numpy()[0]
                buy_confidence = float(buy_probs_b[1]) if buy_action_b == 1 else float(buy_probs_b[0])
            except:
                buy_confidence = 0.5
            
            # Sell æ¨è«–
            sell_features = np.concatenate([features, [1.0]])
            sell_action_b, _ = sell_model_b.predict(sell_features, deterministic=True)
            
            results['strategy_b'] = {
                'name': 'Conservative (MDD -6%)',
                'buy_action': int(buy_action_b),
                'buy_signal': 'BUY' if buy_action_b == 1 else 'HOLD',
                'buy_confidence': buy_confidence,
                'sell_action': int(sell_action_b),
                'sell_signal': 'SELL' if sell_action_b == 1 else 'HOLD',
            }
            print(f"  Buy: {results['strategy_b']['buy_signal']} (Conf: {results['strategy_b']['buy_confidence']:.2%})")
            print(f"  Sell: {results['strategy_b']['sell_signal']}")
            
        except Exception as e:
            print(f"  [Error] æ¨è«–å¤±æ•—: {e}")
            results['strategy_b'] = {'name': 'Conservative', 'error': str(e)}
    else:
        print(f"  [Warning] æ‰¾ä¸åˆ°æ¨¡å‹")
        results['strategy_b'] = {'name': 'Conservative', 'error': 'Model not found'}
    
    return results


# =============================================================================
# Step 4: è¼¸å‡ºæˆ°æƒ…å„€è¡¨æ¿èˆ‡æ—¥èªŒ
# =============================================================================
def generate_report(workspace: dict, df: pd.DataFrame, inference_results: dict, date_str: str):
    """
    è¼¸å‡ºæˆ°æƒ…å„€è¡¨æ¿ä¸¦å„²å­˜æ—¥èªŒ
    
    Args:
        workspace: ç•¶æ—¥å·¥ä½œå€è·¯å¾‘å­—å…¸
        df: ç‰¹å¾µ DataFrame
        inference_results: æ¨è«–çµæœå­—å…¸
        date_str: æ—¥æœŸå­—ä¸²
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š Step 4: æˆ°æƒ…å„€è¡¨æ¿")
    print("=" * 60)
    
    latest = df.iloc[-1]
    
    # å¸‚å ´æ•¸æ“š
    report_lines = []
    report_lines.append("=" * 60)
    report_lines.append(f"  Hybrid Trading System - Daily Report")
    report_lines.append(f"  æ—¥æœŸ: {date_str}")
    report_lines.append(f"  å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("=" * 60)
    
    report_lines.append("\nğŸ“ˆ å¸‚å ´æ•¸æ“š (^TWII)")
    report_lines.append("-" * 40)
    report_lines.append(f"  æ”¶ç›¤åƒ¹:     {latest.get('Close', 0):.2f}")
    report_lines.append(f"  æœ€é«˜åƒ¹:     {latest.get('High', 0):.2f}")
    report_lines.append(f"  æœ€ä½åƒ¹:     {latest.get('Low', 0):.2f}")
    report_lines.append(f"  æˆäº¤é‡:     {latest.get('Volume', 0):,.0f}")
    
    report_lines.append("\nğŸ“Š æŠ€è¡“æŒ‡æ¨™")
    report_lines.append("-" * 40)
    rsi_val = latest.get('RSI', 0)
    mfi_val = latest.get('MFI', 0)
    atr_val = latest.get('ATR', 0)
    dc_val = latest.get('DC_Position', 0)
    
    # RSI/MFI å¯èƒ½å·²ç¶“æ˜¯ 0-1 æˆ– 0-100ï¼Œçµ±ä¸€é¡¯ç¤º
    rsi_display = rsi_val * 100 if rsi_val <= 1 else rsi_val
    mfi_display = mfi_val * 100 if mfi_val <= 1 else mfi_val
    
    report_lines.append(f"  RSI (14):   {rsi_display:.1f}")
    report_lines.append(f"  MFI (14):   {mfi_display:.1f}")
    report_lines.append(f"  ATR:        {atr_val:.2f}")
    report_lines.append(f"  DC ä½ç½®:    {dc_val:.2%}")
    
    report_lines.append("\nğŸ¤– LSTM é æ¸¬")
    report_lines.append("-" * 40)
    lstm_1d = latest.get('LSTM_Pred_1d', 0)
    lstm_5d = latest.get('LSTM_Pred_5d', 0)
    lstm_conf = latest.get('LSTM_Conf_5d', 0.5)
    
    report_lines.append(f"  T+1 é æ¸¬æ¼²å¹…:  {lstm_1d * 100:+.2f}%")
    report_lines.append(f"  T+5 é æ¸¬æ¼²å¹…:  {lstm_5d * 100:+.2f}%")
    report_lines.append(f"  T+5 ä¿¡å¿ƒåº¦:    {lstm_conf * 100:.1f}%")
    
    report_lines.append("\nğŸ¯ ç­–ç•¥å»ºè­°")
    report_lines.append("-" * 40)
    
    # Strategy A
    if 'strategy_a' in inference_results and 'error' not in inference_results['strategy_a']:
        sa = inference_results['strategy_a']
        report_lines.append(f"\n  ã€ç­–ç•¥ A: {sa['name']}ã€‘")
        report_lines.append(f"    è²·å…¥è¨Šè™Ÿ: {sa['buy_signal']} (ä¿¡å¿ƒåº¦: {sa['buy_confidence']:.1%})")
        report_lines.append(f"    è³£å‡ºè¨Šè™Ÿ: {sa['sell_signal']}")
    else:
        error_msg = inference_results.get('strategy_a', {}).get('error', 'æœªçŸ¥éŒ¯èª¤')
        report_lines.append(f"\n  ã€ç­–ç•¥ A: ç„¡æ³•è¼‰å…¥ ({error_msg})ã€‘")
    
    # Strategy B
    if 'strategy_b' in inference_results and 'error' not in inference_results['strategy_b']:
        sb = inference_results['strategy_b']
        report_lines.append(f"\n  ã€ç­–ç•¥ B: {sb['name']}ã€‘")
        report_lines.append(f"    è²·å…¥è¨Šè™Ÿ: {sb['buy_signal']} (ä¿¡å¿ƒåº¦: {sb['buy_confidence']:.1%})")
        report_lines.append(f"    è³£å‡ºè¨Šè™Ÿ: {sb['sell_signal']}")
    else:
        error_msg = inference_results.get('strategy_b', {}).get('error', 'æœªçŸ¥éŒ¯èª¤')
        report_lines.append(f"\n  ã€ç­–ç•¥ B: ç„¡æ³•è¼‰å…¥ ({error_msg})ã€‘")
    
    report_lines.append("\n" + "=" * 60)
    report_lines.append("  å·¥ä½œå€è·¯å¾‘: " + workspace['root'])
    report_lines.append("=" * 60)
    
    # è¼¸å‡ºåˆ°çµ‚ç«¯æ©Ÿ
    report_text = "\n".join(report_lines)
    print(report_text)
    
    # å„²å­˜åˆ°æª”æ¡ˆ
    report_path = os.path.join(workspace['reports'], 'summary.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    # åŒæ™‚å„²å­˜ JSON æ ¼å¼ (æ–¹ä¾¿ç¨‹å¼è®€å–)
    json_path = os.path.join(workspace['reports'], 'summary.json')
    json_data = {
        'date': date_str,
        'generated_at': datetime.now().isoformat(),
        'market_data': {
            'close': float(latest.get('Close', 0)),
            'high': float(latest.get('High', 0)),
            'low': float(latest.get('Low', 0)),
            'volume': float(latest.get('Volume', 0)),
        },
        'indicators': {
            'rsi': float(rsi_display),
            'mfi': float(mfi_display),
            'atr': float(atr_val),
            'dc_position': float(dc_val),
        },
        'lstm_predictions': {
            'pred_1d': float(lstm_1d),
            'pred_5d': float(lstm_5d),
            'conf_5d': float(lstm_conf),
        },
        'strategies': inference_results,
    }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n[Report] å ±å‘Šå·²å„²å­˜:")
    print(f"  - TXT: {report_path}")
    print(f"  - JSON: {json_path}")


# =============================================================================
# Main
# =============================================================================
def main():
    """ä¸»ç¨‹å¼é€²å…¥é»"""
    
    print("\n" + "=" * 70)
    print("  ğŸš€ Daily Operations with Dual Strategy & Versioning (v2)")
    print("=" * 70)
    
    # å–å¾—ä»Šå¤©æ—¥æœŸ
    today = datetime.now()
    date_str = today.strftime('%Y-%m-%d')
    
    # å¦‚æœæ˜¯é€±æœ«ï¼Œä½¿ç”¨ä¸Šä¸€å€‹äº¤æ˜“æ—¥
    if today.weekday() == 5:  # Saturday
        today = today - timedelta(days=1)
        date_str = today.strftime('%Y-%m-%d')
        print(f"[Info] ä»Šå¤©æ˜¯é€±å…­ï¼Œä½¿ç”¨é€±äº”æ—¥æœŸ: {date_str}")
    elif today.weekday() == 6:  # Sunday
        today = today - timedelta(days=2)
        date_str = today.strftime('%Y-%m-%d')
        print(f"[Info] ä»Šå¤©æ˜¯é€±æ—¥ï¼Œä½¿ç”¨é€±äº”æ—¥æœŸ: {date_str}")
    
    print(f"\nğŸ“… åŸ·è¡Œæ—¥æœŸ: {date_str}")
    
    # Step 0: å»ºç«‹ç•¶æ—¥å·¥ä½œå€
    workspace = create_daily_workspace(date_str)
    
    # Step 1: LSTM è¨“ç·´èˆ‡å°å­˜
    train_and_archive_lstm(workspace, date_str)
    
    # Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)
    df = isolated_feature_engineering(workspace, date_str)
    
    # Step 3: é›™æ¨¡å‹æ¨è«–
    inference_results = dual_inference(workspace, df)
    
    # Step 4: è¼¸å‡ºå ±å‘Š
    generate_report(workspace, df, inference_results, date_str)
    
    print("\n" + "=" * 70)
    print("  âœ… Daily Operations å®Œæˆ")
    print("=" * 70)


if __name__ == "__main__":
    main()
