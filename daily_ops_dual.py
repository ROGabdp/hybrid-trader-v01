# -*- coding: utf-8 -*-
"""
================================================================================
Daily Operations with Dual Strategy & Versioning
================================================================================
æ¯æ—¥ç¶­é‹è…³æœ¬ - é›™ç­–ç•¥æ¨è«–èˆ‡ç‰ˆæœ¬æ§ç®¡

åŠŸèƒ½ï¼š
1. å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€ (daily_runs/{date}/)
2. LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜
3. éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (ä½¿ç”¨ç•¶æ—¥æ¨¡å‹)
4. é›™æ¨¡å‹æ¨è«– (Aggressive vs Conservative)
5. è¼¸å‡ºæˆ°æƒ…å„€è¡¨æ¿èˆ‡æ—¥èªŒ

ä½œè€…ï¼šPhil Liang
æ—¥æœŸï¼š2025-12-07
================================================================================
"""

import os
import sys
import shutil
import pickle
from datetime import datetime, timedelta

# è¨­å®š UTF-8 è¼¸å‡º
sys.stdout.reconfigure(encoding='utf-8')

# æŠ‘åˆ¶ TensorFlow è­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import yfinance as yf
from tqdm import tqdm

# =============================================================================
# è¨­å®šè·¯å¾‘
# =============================================================================
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
DAILY_RUNS_PATH = os.path.join(PROJECT_PATH, 'daily_runs')

# RL æ¨¡å‹è·¯å¾‘ (å›ºå®š)
STRATEGY_A_PATH = os.path.join(PROJECT_PATH, 'models_hybrid')  # Aggressive
STRATEGY_B_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v2_conservative')  # Conservative

# LSTM æ¨¡å‹é è¨­è·¯å¾‘
DEFAULT_LSTM_5D_PATH = os.path.join(PROJECT_PATH, 'saved_models_5d')
DEFAULT_LSTM_1D_PATH = os.path.join(PROJECT_PATH, 'saved_models_multivariate')


# =============================================================================
# Step 0: å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€
# =============================================================================
def create_daily_workspace(date_str: str) -> dict:
    """å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€ç›®éŒ„çµæ§‹"""
    
    daily_path = os.path.join(DAILY_RUNS_PATH, date_str)
    
    paths = {
        'root': daily_path,
        'lstm_models': os.path.join(daily_path, 'lstm_models'),
        'cache': os.path.join(daily_path, 'cache'),
        'reports': os.path.join(daily_path, 'reports'),
    }
    
    for key, path in paths.items():
        os.makedirs(path, exist_ok=True)
    
    print(f"[Workspace] å»ºç«‹ç•¶æ—¥å·¥ä½œå€: {daily_path}")
    return paths


# =============================================================================
# Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜
# =============================================================================
def train_and_archive_lstm(workspace: dict, end_date: str):
    """
    è¨“ç·´ LSTM æ¨¡å‹ä¸¦å°å­˜åˆ°ç•¶æ—¥å·¥ä½œå€
    
    Args:
        workspace: ç•¶æ—¥å·¥ä½œå€è·¯å¾‘å­—å…¸
        end_date: è¨“ç·´çµæŸæ—¥æœŸ (YYYY-MM-DD)
    """
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜")
    print("=" * 60)
    
    # å‹•æ…‹å¼•å…¥æ¨¡å‹è¨“ç·´æ¨¡çµ„
    try:
        import twii_model_registry_5d as registry_5d
        import twii_model_registry_multivariate as registry_1d
    except ImportError as e:
        print(f"[Error] ç„¡æ³•è¼‰å…¥ LSTM æ¨¡çµ„: {e}")
        return False
    
    start_date = "2000-01-01"
    
    # =========================================================================
    # è¨“ç·´ T+5 æ¨¡å‹
    # =========================================================================
    print(f"\n[LSTM T+5] è¨“ç·´ç¯„åœ: {start_date} ~ {end_date}")
    try:
        # ä¸‹è¼‰æ•¸æ“š
        df_5d = yf.download("^TWII", start=start_date, end=end_date, auto_adjust=True, progress=False)
        if len(df_5d) < 100:
            print("[Error] æ•¸æ“šä¸è¶³ï¼Œè·³é T+5 è¨“ç·´")
        else:
            # è¨“ç·´æ¨¡å‹
            registry_5d.train_model(df_5d, start_date, end_date)
            print("[LSTM T+5] âœ… è¨“ç·´å®Œæˆ")
    except Exception as e:
        print(f"[LSTM T+5] è¨“ç·´å¤±æ•—: {e}")
    
    # =========================================================================
    # è¨“ç·´ T+1 æ¨¡å‹
    # =========================================================================
    print(f"\n[LSTM T+1] è¨“ç·´ç¯„åœ: {start_date} ~ {end_date}")
    try:
        # ä¸‹è¼‰æ•¸æ“š
        df_1d = yf.download("^TWII", start=start_date, end=end_date, auto_adjust=True, progress=False)
        if len(df_1d) < 100:
            print("[Error] æ•¸æ“šä¸è¶³ï¼Œè·³é T+1 è¨“ç·´")
        else:
            # è¨“ç·´æ¨¡å‹
            registry_1d.train_model(df_1d, start_date, end_date)
            print("[LSTM T+1] âœ… è¨“ç·´å®Œæˆ")
    except Exception as e:
        print(f"[LSTM T+1] è¨“ç·´å¤±æ•—: {e}")
    
    # =========================================================================
    # å°å­˜æ¨¡å‹åˆ°ç•¶æ—¥å·¥ä½œå€
    # =========================================================================
    print("\n[Archive] å°å­˜æ¨¡å‹åˆ°ç•¶æ—¥å·¥ä½œå€...")
    
    archive_path = workspace['lstm_models']
    
    # è¤‡è£½ T+5 æ¨¡å‹
    for src_dir in [DEFAULT_LSTM_5D_PATH]:
        if os.path.exists(src_dir):
            dest_dir = os.path.join(archive_path, os.path.basename(src_dir))
            if os.path.exists(dest_dir):
                shutil.rmtree(dest_dir)
            shutil.copytree(src_dir, dest_dir)
            print(f"  âœ… å·²è¤‡è£½: {os.path.basename(src_dir)}")
    
    # è¤‡è£½ T+1 æ¨¡å‹
    for src_dir in [DEFAULT_LSTM_1D_PATH]:
        if os.path.exists(src_dir):
            dest_dir = os.path.join(archive_path, os.path.basename(src_dir))
            if os.path.exists(dest_dir):
                shutil.rmtree(dest_dir)
            shutil.copytree(src_dir, dest_dir)
            print(f"  âœ… å·²è¤‡è£½: {os.path.basename(src_dir)}")
    
    return True


# =============================================================================
# Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹
# =============================================================================
def isolated_feature_engineering(workspace: dict, end_date: str) -> pd.DataFrame:
    """
    ä½¿ç”¨ç•¶æ—¥å°å­˜çš„ LSTM æ¨¡å‹é€²è¡Œç‰¹å¾µå·¥ç¨‹
    
    Args:
        workspace: ç•¶æ—¥å·¥ä½œå€è·¯å¾‘å­—å…¸
        end_date: æ•¸æ“šçµæŸæ—¥æœŸ
    
    Returns:
        åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„ DataFrame
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹")
    print("=" * 60)
    
    import tensorflow as tf
    from tensorflow import keras
    from keras import layers
    import ta
    
    # è‡ªè¨‚ SelfAttention å±¤ (èˆ‡åŸå§‹æ¨¡å‹ç›¸åŒ)
    class SelfAttention(layers.Layer):
        def __init__(self, **kwargs):
            super(SelfAttention, self).__init__(**kwargs)
        
        def build(self, input_shape):
            self.units = input_shape[-1]
            self.W_q = self.add_weight(name='W_query', shape=(self.units, self.units),
                                       initializer='glorot_uniform', trainable=True)
            self.W_k = self.add_weight(name='W_key', shape=(self.units, self.units),
                                       initializer='glorot_uniform', trainable=True)
        
        def call(self, inputs, training=None):
            Q = tf.matmul(inputs, self.W_q)
            K = tf.matmul(inputs, self.W_k)
            attention = tf.nn.softmax(tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.units, tf.float32)))
            return tf.matmul(attention, inputs)
    
    # =========================================================================
    # è¼‰å…¥ç•¶æ—¥å°å­˜çš„ LSTM æ¨¡å‹
    # =========================================================================
    lstm_5d_path = os.path.join(workspace['lstm_models'], 'saved_models_5d')
    lstm_1d_path = os.path.join(workspace['lstm_models'], 'saved_models_multivariate')
    
    model_5d, scaler_5d, meta_5d = None, None, None
    model_1d, scaler_1d, meta_1d = None, None, None
    
    # è¼‰å…¥ T+5 æ¨¡å‹
    if os.path.exists(lstm_5d_path):
        import glob
        import json
        
        keras_files = glob.glob(os.path.join(lstm_5d_path, "*.keras"))
        if keras_files:
            latest_keras = sorted(keras_files)[-1]
            model_5d = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})
            
            # è¼‰å…¥ scaler
            scaler_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
            if os.path.exists(scaler_file):
                with open(scaler_file, 'rb') as f:
                    scaler_5d = pickle.load(f)
            
            # è¼‰å…¥ meta
            meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
            if os.path.exists(meta_file):
                with open(meta_file, 'r') as f:
                    meta_5d = json.load(f)
            
            print(f"[LSTM T+5] âœ… å·²è¼‰å…¥: {os.path.basename(latest_keras)}")
    
    # è¼‰å…¥ T+1 æ¨¡å‹
    if os.path.exists(lstm_1d_path):
        import glob
        import json
        
        keras_files = glob.glob(os.path.join(lstm_1d_path, "*.keras"))
        if keras_files:
            latest_keras = sorted(keras_files)[-1]
            model_1d = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})
            
            # è¼‰å…¥ scaler
            scaler_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
            if os.path.exists(scaler_file):
                with open(scaler_file, 'rb') as f:
                    scaler_1d = pickle.load(f)
            
            # è¼‰å…¥ meta
            meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
            if os.path.exists(meta_file):
                with open(meta_file, 'r') as f:
                    meta_1d = json.load(f)
            
            print(f"[LSTM T+1] âœ… å·²è¼‰å…¥: {os.path.basename(latest_keras)}")
    
    # =========================================================================
    # ä¸‹è¼‰æœ€æ–°æ•¸æ“š
    # =========================================================================
    print("\n[Data] ä¸‹è¼‰ ^TWII æ•¸æ“š...")
    df = yf.download("^TWII", start="2020-01-01", end=end_date, auto_adjust=True, progress=False)
    
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)
    
    print(f"[Data] æ•¸æ“šç¯„åœ: {df.index[0].strftime('%Y-%m-%d')} ~ {df.index[-1].strftime('%Y-%m-%d')}")
    print(f"[Data] ç¸½ç­†æ•¸: {len(df)}")
    
    # =========================================================================
    # è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
    # =========================================================================
    print("\n[Features] è¨ˆç®—æŠ€è¡“æŒ‡æ¨™...")
    
    # åŸºç¤åƒ¹æ ¼æŒ‡æ¨™
    df['Norm_Open'] = df['Open'] / df['Close'].rolling(20).mean()
    df['Norm_High'] = df['High'] / df['Close'].rolling(20).mean()
    df['Norm_Low'] = df['Low'] / df['Close'].rolling(20).mean()
    df['Norm_Close'] = df['Close'] / df['Close'].rolling(20).mean()
    
    # Donchian Channel
    df['DC_High'] = df['High'].rolling(20).max()
    df['DC_Low'] = df['Low'].rolling(20).min()
    df['DC_Position'] = (df['Close'] - df['DC_Low']) / (df['DC_High'] - df['DC_Low'] + 1e-8)
    
    # SuperTrend (ç°¡åŒ–ç‰ˆ)
    df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], window=10)
    df['SuperTrend_Signal'] = np.where(df['Close'] > df['Close'].rolling(10).mean() + df['ATR'], 1,
                                        np.where(df['Close'] < df['Close'].rolling(10).mean() - df['ATR'], -1, 0))
    
    # Heikin-Ashi
    df['HA_Close'] = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4
    df['HA_Open'] = (df['Open'].shift(1) + df['Close'].shift(1)) / 2
    df['HA_Trend'] = np.where(df['HA_Close'] > df['HA_Open'], 1, -1)
    
    # RSI, MFI
    df['RSI'] = ta.momentum.rsi(df['Close'], window=14) / 100
    df['MFI'] = ta.volume.money_flow_index(df['High'], df['Low'], df['Close'], df['Volume'], window=14) / 100
    
    # MA
    df['MA10'] = df['Close'].rolling(10).mean()
    df['MA20'] = df['Close'].rolling(20).mean()
    df['MA60'] = df['Close'].rolling(60).mean()
    df['MA_Ratio_10_60'] = df['MA10'] / df['MA60']
    
    # Relative Strength
    df['RS_5d'] = df['Close'].pct_change(5)
    df['RS_20d'] = df['Close'].pct_change(20)
    
    # Volume
    df['Vol_Ratio'] = df['Volume'] / df['Volume'].rolling(20).mean()
    
    # =========================================================================
    # è¨ˆç®— LSTM é æ¸¬ç‰¹å¾µ
    # =========================================================================
    print("\n[LSTM] è¨ˆç®—é æ¸¬ç‰¹å¾µ...")
    
    df['LSTM_Pred_1d'] = 0.0
    df['LSTM_Pred_5d'] = 0.0
    df['LSTM_Conf_5d'] = 0.5
    
    # LSTM ç‰¹å¾µæ¬„ä½
    lstm_features = ['Close', 'Volume', 'RSI', 'MFI']
    
    if model_5d is not None and scaler_5d is not None:
        LOOKBACK_5D = 30
        
        for i in range(LOOKBACK_5D, len(df)):
            try:
                window = df.iloc[i-LOOKBACK_5D:i][['Close', 'Volume']].copy()
                window['Volume'] = np.log1p(window['Volume'])
                
                # æ·»åŠ  KD, MACD_Hist
                window['KD'] = df['RSI'].iloc[i-LOOKBACK_5D:i].values * 100
                window['MACD_Hist'] = df['Close'].iloc[i-LOOKBACK_5D:i].pct_change().fillna(0).values
                
                scaled = scaler_5d.transform(window.values)
                X = scaled.reshape(1, LOOKBACK_5D, -1)
                
                # MC Dropout é æ¸¬
                preds = []
                for _ in range(5):
                    pred = model_5d(X, training=True).numpy()[0, 0]
                    preds.append(pred)
                
                mean_pred = np.mean(preds)
                std_pred = np.std(preds)
                
                # åæ­£è¦åŒ–
                price_min = meta_5d.get('price_min', df['Close'].min())
                price_max = meta_5d.get('price_max', df['Close'].max())
                pred_price = mean_pred * (price_max - price_min) + price_min
                current_price = df['Close'].iloc[i]
                
                df.iloc[i, df.columns.get_loc('LSTM_Pred_5d')] = (pred_price - current_price) / current_price
                df.iloc[i, df.columns.get_loc('LSTM_Conf_5d')] = max(0, min(1, 1 - std_pred * 10))
                
            except Exception as e:
                pass
    
    if model_1d is not None and scaler_1d is not None:
        LOOKBACK_1D = 10
        
        for i in range(LOOKBACK_1D, len(df)):
            try:
                window = df.iloc[i-LOOKBACK_1D:i][['Close', 'Volume']].copy()
                window['Volume'] = np.log1p(window['Volume'])
                window['KD'] = df['RSI'].iloc[i-LOOKBACK_1D:i].values * 100
                window['MACD_Hist'] = df['Close'].iloc[i-LOOKBACK_1D:i].pct_change().fillna(0).values
                
                scaled = scaler_1d.transform(window.values)
                X = scaled.reshape(1, LOOKBACK_1D, -1)
                
                pred = model_1d.predict(X, verbose=0)[0, 0]
                
                price_min = meta_1d.get('price_min', df['Close'].min())
                price_max = meta_1d.get('price_max', df['Close'].max())
                pred_price = pred * (price_max - price_min) + price_min
                current_price = df['Close'].iloc[i]
                
                df.iloc[i, df.columns.get_loc('LSTM_Pred_1d')] = (pred_price - current_price) / current_price
                
            except Exception as e:
                pass
    
    # =========================================================================
    # å„²å­˜å¿«å–
    # =========================================================================
    df = df.dropna()
    
    cache_path = os.path.join(workspace['cache'], 'twii_features.pkl')
    with open(cache_path, 'wb') as f:
        pickle.dump(df, f)
    
    print(f"\n[Cache] ç‰¹å¾µå·²å„²å­˜: {cache_path}")
    print(f"[Cache] æœ€çµ‚ç­†æ•¸: {len(df)}")
    
    return df


# =============================================================================
# Step 3: é›™æ¨¡å‹æ¨è«–
# =============================================================================
def dual_inference(workspace: dict, df: pd.DataFrame) -> dict:
    """
    ä½¿ç”¨å…©å¥— RL æ¨¡å‹é€²è¡Œæ¨è«–
    
    Args:
        workspace: ç•¶æ—¥å·¥ä½œå€è·¯å¾‘å­—å…¸
        df: ç‰¹å¾µ DataFrame
    
    Returns:
        åŒ…å«å…©å¥—ç­–ç•¥å»ºè­°çš„å­—å…¸
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ Step 3: é›™æ¨¡å‹æ¨è«–")
    print("=" * 60)
    
    from stable_baselines3 import PPO
    
    # ç‰¹å¾µæ¬„ä½ (èˆ‡è¨“ç·´æ™‚ç›¸åŒ)
    FEATURE_COLS = [
        'Norm_Open', 'Norm_High', 'Norm_Low', 'Norm_Close',
        'DC_Position', 'SuperTrend_Signal', 'HA_Trend',
        'RSI', 'MFI', 'ATR', 'MA_Ratio_10_60',
        'RS_5d', 'RS_20d', 'Vol_Ratio',
        'LSTM_Pred_1d', 'LSTM_Pred_5d', 'LSTM_Conf_5d'
    ]
    
    # å–å¾—æœ€æ–°ä¸€ç­†æ•¸æ“š
    latest = df.iloc[-1]
    
    # æº–å‚™ç‰¹å¾µå‘é‡
    available_cols = [c for c in FEATURE_COLS if c in df.columns]
    features = latest[available_cols].values.astype(np.float32)
    
    # è£œé½Šç¼ºå¤±çš„æ¬„ä½
    if len(features) < 23:
        features = np.pad(features, (0, 23 - len(features)), mode='constant', constant_values=0)
    
    results = {}
    
    # =========================================================================
    # Strategy A: Aggressive (ROI 85%)
    # =========================================================================
    print("\n[Strategy A] Aggressive (ROI 85%)...")
    
    buy_a_path = os.path.join(STRATEGY_A_PATH, 'ppo_buy_twii_final.zip')
    sell_a_path = os.path.join(STRATEGY_A_PATH, 'ppo_sell_twii_final.zip')
    
    if os.path.exists(buy_a_path) and os.path.exists(sell_a_path):
        buy_model_a = PPO.load(buy_a_path)
        sell_model_a = PPO.load(sell_a_path)
        
        # Buy æ¨è«–
        buy_action_a, _ = buy_model_a.predict(features, deterministic=True)
        buy_probs_a = buy_model_a.policy.get_distribution(
            buy_model_a.policy.obs_to_tensor(features.reshape(1, -1))[0]
        ).distribution.probs.detach().numpy()[0]
        
        # Sell æ¨è«– (éœ€è¦åŠ å…¥æŒæœ‰å ±é…¬)
        sell_features = np.concatenate([features, [1.0]])  # å‡è¨­æŒæœ‰å ±é…¬ 0%
        sell_action_a, _ = sell_model_a.predict(sell_features, deterministic=True)
        
        results['strategy_a'] = {
            'name': 'Aggressive (ROI 85%)',
            'buy_action': int(buy_action_a),
            'buy_signal': 'BUY' if buy_action_a == 1 else 'HOLD',
            'buy_confidence': float(buy_probs_a[1]) if buy_action_a == 1 else float(buy_probs_a[0]),
            'sell_action': int(sell_action_a),
            'sell_signal': 'SELL' if sell_action_a == 1 else 'HOLD',
        }
        print(f"  Buy: {results['strategy_a']['buy_signal']} (Conf: {results['strategy_a']['buy_confidence']:.2%})")
        print(f"  Sell: {results['strategy_a']['sell_signal']}")
    else:
        print(f"  [Warning] æ‰¾ä¸åˆ°æ¨¡å‹: {buy_a_path}")
        results['strategy_a'] = {'name': 'Aggressive', 'error': 'Model not found'}
    
    # =========================================================================
    # Strategy B: Conservative (MDD -6%)
    # =========================================================================
    print("\n[Strategy B] Conservative (MDD -6%)...")
    
    buy_b_path = os.path.join(STRATEGY_B_PATH, 'ppo_buy_twii_final.zip')
    sell_b_path = os.path.join(STRATEGY_B_PATH, 'ppo_sell_twii_final.zip')
    
    if os.path.exists(buy_b_path) and os.path.exists(sell_b_path):
        buy_model_b = PPO.load(buy_b_path)
        sell_model_b = PPO.load(sell_b_path)
        
        # Buy æ¨è«–
        buy_action_b, _ = buy_model_b.predict(features, deterministic=True)
        buy_probs_b = buy_model_b.policy.get_distribution(
            buy_model_b.policy.obs_to_tensor(features.reshape(1, -1))[0]
        ).distribution.probs.detach().numpy()[0]
        
        # Sell æ¨è«–
        sell_features = np.concatenate([features, [1.0]])
        sell_action_b, _ = sell_model_b.predict(sell_features, deterministic=True)
        
        results['strategy_b'] = {
            'name': 'Conservative (MDD -6%)',
            'buy_action': int(buy_action_b),
            'buy_signal': 'BUY' if buy_action_b == 1 else 'HOLD',
            'buy_confidence': float(buy_probs_b[1]) if buy_action_b == 1 else float(buy_probs_b[0]),
            'sell_action': int(sell_action_b),
            'sell_signal': 'SELL' if sell_action_b == 1 else 'HOLD',
        }
        print(f"  Buy: {results['strategy_b']['buy_signal']} (Conf: {results['strategy_b']['buy_confidence']:.2%})")
        print(f"  Sell: {results['strategy_b']['sell_signal']}")
    else:
        print(f"  [Warning] æ‰¾ä¸åˆ°æ¨¡å‹: {buy_b_path}")
        results['strategy_b'] = {'name': 'Conservative', 'error': 'Model not found'}
    
    return results


# =============================================================================
# Step 4: è¼¸å‡ºæˆ°æƒ…å„€è¡¨æ¿èˆ‡æ—¥èªŒ
# =============================================================================
def generate_report(workspace: dict, df: pd.DataFrame, inference_results: dict, date_str: str):
    """
    è¼¸å‡ºæˆ°æƒ…å„€è¡¨æ¿ä¸¦å„²å­˜æ—¥èªŒ
    
    Args:
        workspace: ç•¶æ—¥å·¥ä½œå€è·¯å¾‘å­—å…¸
        df: ç‰¹å¾µ DataFrame
        inference_results: æ¨è«–çµæœå­—å…¸
        date_str: æ—¥æœŸå­—ä¸²
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š Step 4: æˆ°æƒ…å„€è¡¨æ¿")
    print("=" * 60)
    
    latest = df.iloc[-1]
    
    # å¸‚å ´æ•¸æ“š
    report_lines = []
    report_lines.append("=" * 60)
    report_lines.append(f"  Hybrid Trading System - Daily Report")
    report_lines.append(f"  æ—¥æœŸ: {date_str}")
    report_lines.append(f"  å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("=" * 60)
    
    report_lines.append("\nğŸ“ˆ å¸‚å ´æ•¸æ“š (^TWII)")
    report_lines.append("-" * 40)
    report_lines.append(f"  æ”¶ç›¤åƒ¹:     {latest['Close']:.2f}")
    report_lines.append(f"  æœ€é«˜åƒ¹:     {latest['High']:.2f}")
    report_lines.append(f"  æœ€ä½åƒ¹:     {latest['Low']:.2f}")
    report_lines.append(f"  æˆäº¤é‡:     {latest['Volume']:,.0f}")
    
    report_lines.append("\nğŸ“Š æŠ€è¡“æŒ‡æ¨™")
    report_lines.append("-" * 40)
    report_lines.append(f"  RSI (14):   {latest.get('RSI', 0) * 100:.1f}")
    report_lines.append(f"  MFI (14):   {latest.get('MFI', 0) * 100:.1f}")
    report_lines.append(f"  ATR:        {latest.get('ATR', 0):.2f}")
    report_lines.append(f"  DC ä½ç½®:    {latest.get('DC_Position', 0):.2%}")
    
    report_lines.append("\nğŸ¤– LSTM é æ¸¬")
    report_lines.append("-" * 40)
    report_lines.append(f"  T+1 é æ¸¬æ¼²å¹…:  {latest.get('LSTM_Pred_1d', 0) * 100:+.2f}%")
    report_lines.append(f"  T+5 é æ¸¬æ¼²å¹…:  {latest.get('LSTM_Pred_5d', 0) * 100:+.2f}%")
    report_lines.append(f"  T+5 ä¿¡å¿ƒåº¦:    {latest.get('LSTM_Conf_5d', 0.5) * 100:.1f}%")
    
    report_lines.append("\nğŸ¯ ç­–ç•¥å»ºè­°")
    report_lines.append("-" * 40)
    
    # Strategy A
    if 'strategy_a' in inference_results and 'error' not in inference_results['strategy_a']:
        sa = inference_results['strategy_a']
        report_lines.append(f"\n  ã€ç­–ç•¥ A: {sa['name']}ã€‘")
        report_lines.append(f"    è²·å…¥è¨Šè™Ÿ: {sa['buy_signal']} (ä¿¡å¿ƒåº¦: {sa['buy_confidence']:.1%})")
        report_lines.append(f"    è³£å‡ºè¨Šè™Ÿ: {sa['sell_signal']}")
    else:
        report_lines.append("\n  ã€ç­–ç•¥ A: ç„¡æ³•è¼‰å…¥ã€‘")
    
    # Strategy B
    if 'strategy_b' in inference_results and 'error' not in inference_results['strategy_b']:
        sb = inference_results['strategy_b']
        report_lines.append(f"\n  ã€ç­–ç•¥ B: {sb['name']}ã€‘")
        report_lines.append(f"    è²·å…¥è¨Šè™Ÿ: {sb['buy_signal']} (ä¿¡å¿ƒåº¦: {sb['buy_confidence']:.1%})")
        report_lines.append(f"    è³£å‡ºè¨Šè™Ÿ: {sb['sell_signal']}")
    else:
        report_lines.append("\n  ã€ç­–ç•¥ B: ç„¡æ³•è¼‰å…¥ã€‘")
    
    report_lines.append("\n" + "=" * 60)
    
    # è¼¸å‡ºåˆ°çµ‚ç«¯æ©Ÿ
    report_text = "\n".join(report_lines)
    print(report_text)
    
    # å„²å­˜åˆ°æª”æ¡ˆ
    report_path = os.path.join(workspace['reports'], 'summary.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print(f"\n[Report] å ±å‘Šå·²å„²å­˜: {report_path}")


# =============================================================================
# Main
# =============================================================================
def main():
    """ä¸»ç¨‹å¼é€²å…¥é»"""
    
    print("\n" + "=" * 70)
    print("  ğŸš€ Daily Operations with Dual Strategy & Versioning")
    print("=" * 70)
    
    # å–å¾—ä»Šå¤©æ—¥æœŸ
    today = datetime.now()
    date_str = today.strftime('%Y-%m-%d')
    
    # å¦‚æœæ˜¯é€±æœ«ï¼Œä½¿ç”¨ä¸Šä¸€å€‹äº¤æ˜“æ—¥
    if today.weekday() == 5:  # Saturday
        today = today - timedelta(days=1)
        date_str = today.strftime('%Y-%m-%d')
        print(f"[Info] ä»Šå¤©æ˜¯é€±å…­ï¼Œä½¿ç”¨é€±äº”æ—¥æœŸ: {date_str}")
    elif today.weekday() == 6:  # Sunday
        today = today - timedelta(days=2)
        date_str = today.strftime('%Y-%m-%d')
        print(f"[Info] ä»Šå¤©æ˜¯é€±æ—¥ï¼Œä½¿ç”¨é€±äº”æ—¥æœŸ: {date_str}")
    
    print(f"\nğŸ“… åŸ·è¡Œæ—¥æœŸ: {date_str}")
    
    # Step 0: å»ºç«‹ç•¶æ—¥å·¥ä½œå€
    workspace = create_daily_workspace(date_str)
    
    # Step 1: LSTM è¨“ç·´èˆ‡å°å­˜
    train_and_archive_lstm(workspace, date_str)
    
    # Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹
    df = isolated_feature_engineering(workspace, date_str)
    
    # Step 3: é›™æ¨¡å‹æ¨è«–
    inference_results = dual_inference(workspace, df)
    
    # Step 4: è¼¸å‡ºå ±å‘Š
    generate_report(workspace, df, inference_results, date_str)
    
    print("\n" + "=" * 70)
    print("  âœ… Daily Operations å®Œæˆ")
    print("=" * 70)


if __name__ == "__main__":
    main()
