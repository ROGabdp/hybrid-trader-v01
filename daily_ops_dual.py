# -*- coding: utf-8 -*-
"""
================================================================================
Daily Operations with Dual Strategy & Versioning (v2.1 - Patched)
================================================================================
æ¯æ—¥ç¶­é‹è…³æœ¬ - é›™ç­–ç•¥æ¨è«–èˆ‡ç‰ˆæœ¬æ§ç®¡

ä¿®æ­£ç´€éŒ„ (v2.2):
1. [Fix] Step 1 æ”¹ç‚ºç›´æ¥å‘¼å« model registry è…³æœ¬ï¼Œä¸¦å‚³å…¥å‹•æ…‹æ—¥æœŸ (ç¢ºä¿æ¨¡å‹æ›´æ–°è‡³ä»Šæ—¥)
2. [Fix] Step 2 è£œä¸Š target_scaler çš„è¼‰å…¥èˆ‡æ³¨å…¥ (é˜²æ­¢ inverse_transform å¤±æ•—)
3. [Safety] å¢åŠ  import æª¢æŸ¥èˆ‡éŒ¯èª¤è™•ç†
4. [Fix] yfinance end_date åŠ ä¸€å¤© (å› ç‚º yf.download çš„ end æ˜¯ exclusive)
5. [Fix] ä½¿ç”¨å¯¦éš›ä¸‹è¼‰è³‡æ–™çš„æœ€å¾Œæ—¥æœŸä½œç‚ºå·¥ä½œå€æ—¥æœŸ (é¿å…é€±æœ«/ç›¤ä¸­åŸ·è¡Œæ™‚æ—¥æœŸä¸ç¬¦)
6. [Safety] meta.json è¼‰å…¥åŠ ä¸Š try-except é˜²è­·

ä½œè€…ï¼šPhil Liang (Fixed by Gemini)
æ—¥æœŸï¼š2025-12-07 (v2.2 Updated)
================================================================================
"""

import os
import sys
import shutil
import pickle
import subprocess
import json
import glob
from datetime import datetime, timedelta

# è¨­å®š UTF-8 è¼¸å‡º
sys.stdout.reconfigure(encoding='utf-8')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import yfinance as yf
from tensorflow import keras
from keras import layers

# =============================================================================
# å¼•ç”¨ä¸»ç³»çµ±
# =============================================================================
import ptrl_hybrid_system as core_system

# =============================================================================
# è¨­å®šè·¯å¾‘
# =============================================================================
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
DAILY_RUNS_PATH = os.path.join(PROJECT_PATH, 'daily_runs')

# RL æ¨¡å‹è·¯å¾‘
STRATEGY_A_PATH = os.path.join(PROJECT_PATH, 'models_hybrid')  # Aggressive
STRATEGY_B_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v2_conservative')  # Conservative

# LSTM è¨“ç·´è…³æœ¬åç¨± (å¿…é ˆå­˜åœ¨æ–¼åŒä¸€ç›®éŒ„ä¸‹)
SCRIPT_5D = "twii_model_registry_5d.py"
SCRIPT_1D = "twii_model_registry_multivariate.py"

# LSTM æ¨¡å‹é è¨­è¼¸å‡ºè·¯å¾‘ (è¨“ç·´è…³æœ¬é è¨­æœƒå­˜åˆ°é€™è£¡)
DEFAULT_LSTM_5D_DIR = os.path.join(PROJECT_PATH, 'saved_models_5d')
DEFAULT_LSTM_1D_DIR = os.path.join(PROJECT_PATH, 'saved_models_multivariate')


# =============================================================================
# Step 0: å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€
# =============================================================================
def create_daily_workspace(date_str: str) -> dict:
    daily_path = os.path.join(DAILY_RUNS_PATH, date_str)
    paths = {
        'root': daily_path,
        'lstm_models': os.path.join(daily_path, 'lstm_models'),
        'lstm_5d': os.path.join(daily_path, 'lstm_models', 'saved_models_5d'),
        'lstm_1d': os.path.join(daily_path, 'lstm_models', 'saved_models_multivariate'),
        'cache': os.path.join(daily_path, 'cache'),
        'reports': os.path.join(daily_path, 'reports'),
    }
    for key, path in paths.items():
        os.makedirs(path, exist_ok=True)
    print(f"[Workspace] å»ºç«‹ç•¶æ—¥å·¥ä½œå€: {daily_path}")
    return paths


# =============================================================================
# Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜
# =============================================================================
def train_and_archive_lstm(workspace: dict, end_date: str):
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜")
    print("=" * 60)
    
    start_date = "2000-01-01"
    
    # 1. åŸ·è¡Œ T+5 è¨“ç·´ (å‚³å…¥å‹•æ…‹æ—¥æœŸ)
    print(f"\n[Training] T+5 Model ({start_date} ~ {end_date})...")
    script_5d_path = os.path.join(PROJECT_PATH, SCRIPT_5D)
    cmd_5d = [sys.executable, script_5d_path, "train", "--start", start_date, "--end", end_date]
    try:
        subprocess.run(cmd_5d, check=True, timeout=1200, cwd=PROJECT_PATH)  # ç¢ºä¿å·¥ä½œç›®éŒ„æ­£ç¢º
        print("[Training] âœ… T+5 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+5 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_5d_path}")
        return False
    except Exception as e:
        print(f"[Error] åŸ·è¡ŒéŒ¯èª¤: {e}")
        return False

    # 2. åŸ·è¡Œ T+1 è¨“ç·´ (å‚³å…¥å‹•æ…‹æ—¥æœŸ)
    print(f"\n[Training] T+1 Model ({start_date} ~ {end_date})...")
    script_1d_path = os.path.join(PROJECT_PATH, SCRIPT_1D)
    cmd_1d = [sys.executable, script_1d_path, "train", "--start", start_date, "--end", end_date]
    try:
        subprocess.run(cmd_1d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+1 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+1 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_1d_path}")
        return False

    # 3. å°å­˜æ¨¡å‹ (Copy from default dir to daily dir)
    print("\n[Archive] å°å­˜æ¨¡å‹åˆ°ç•¶æ—¥å·¥ä½œå€...")
    
    def archive_dir(src_dir, dest_dir):
        if os.path.exists(src_dir):
            if os.path.exists(dest_dir):
                shutil.rmtree(dest_dir) # æ¸…ç©ºèˆŠçš„
            shutil.copytree(src_dir, dest_dir)
            print(f"  âœ… å·²å°å­˜: {os.path.basename(src_dir)} -> {dest_dir}")
        else:
            print(f"  âš ï¸ ä¾†æºç›®éŒ„ä¸å­˜åœ¨: {src_dir}")

    archive_dir(DEFAULT_LSTM_5D_DIR, workspace['lstm_5d'])
    archive_dir(DEFAULT_LSTM_1D_DIR, workspace['lstm_1d'])
    
    return True


# =============================================================================
# Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (ä¿®æ­£ç‰ˆ)
# =============================================================================
def isolated_feature_engineering(workspace: dict, end_date: str) -> pd.DataFrame:
    print("\n" + "=" * 60)
    print("ğŸ”§ Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)")
    print("=" * 60)
    
    # [ä¿®æ­£] ç›´æ¥å¾åŸå§‹è¨“ç·´è…³æœ¬å¼•ç”¨æ­£ç¢ºçš„ Layer å®šç¾©
    # é€™æ¨£ç¢ºä¿æ•¸å­¸é‹ç®—é‚è¼¯ (Attention Score è¨ˆç®—) èˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´
    try:
        from twii_model_registry_5d import SelfAttention
        print("[System] æˆåŠŸå¼•ç”¨åŸå§‹ SelfAttention é¡åˆ¥")
    except ImportError:
        print("[Error] ç„¡æ³•å¼•ç”¨ twii_model_registry_5dï¼Œè«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å­˜åœ¨")
        sys.exit(1)

    # è¼”åŠ©å‡½å¼ï¼šè¼‰å…¥æ•´çµ„æ¨¡å‹å…ƒä»¶
    def load_model_components(model_dir):
        keras_files = glob.glob(os.path.join(model_dir, "*.keras"))
        if not keras_files: return None, None, None, None
        
        # æ‰¾æœ€æ–°çš„æ¨¡å‹æª”
        latest_keras = sorted(keras_files)[-1]
        print(f"  ...Loading {os.path.basename(latest_keras)}")
        
        # [ä¿®æ­£] è¼‰å…¥æ¨¡å‹æ™‚ä½¿ç”¨æ­£ç¢ºçš„ Custom Object
        model = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})

        # è¼‰å…¥ Meta (åŠ ä¸ŠéŒ¯èª¤é˜²è­·)
        meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
        meta = {}
        if os.path.exists(meta_file):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
            except Exception as e:
                print(f"  âš ï¸ è¼‰å…¥ meta å¤±æ•—: {e}")

        # è¼‰å…¥ Feature Scaler
        scaler_feat_file = latest_keras.replace('model_', 'feature_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_feat_file):
             scaler_feat_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
        
        scaler_feat = None
        if os.path.exists(scaler_feat_file):
            with open(scaler_feat_file, 'rb') as f:
                scaler_feat = pickle.load(f)

        # è¼‰å…¥ Target Scaler
        scaler_tgt_file = latest_keras.replace('model_', 'target_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_tgt_file):
             scaler_tgt = scaler_feat
        else:
             with open(scaler_tgt_file, 'rb') as f:
                 scaler_tgt = pickle.load(f)

        return model, scaler_feat, scaler_tgt, meta

    # 1. è¼‰å…¥æ¨¡å‹
    print("\n[Model Injection] è¼‰å…¥ç•¶æ—¥å°å­˜çš„ LSTM æ¨¡å‹...")
    m5d, sf5d, st5d, meta5d = load_model_components(workspace['lstm_5d'])
    m1d, sf1d, st1d, meta1d = load_model_components(workspace['lstm_1d'])
    
    if m5d is None or m1d is None:
        print("[Error] æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œç‰¹å¾µå·¥ç¨‹")
        sys.exit(1)

    # 2. æ³¨å…¥ä¸»ç³»çµ±
    print("\n[Model Injection] æ³¨å…¥ core_system._LSTM_MODELS...")
    if not hasattr(core_system, '_LSTM_MODELS'):
        core_system._LSTM_MODELS = {}
    
    core_system._LSTM_MODELS.update({
        'model_5d': m5d, 'scaler_feat_5d': sf5d, 'scaler_tgt_5d': st5d, 'meta_5d': meta5d,
        'model_1d': m1d, 'scaler_feat_1d': sf1d, 'scaler_tgt_1d': st1d, 'meta_1d': meta1d,
        'loaded': True
    })
    print("  âœ… æ³¨å…¥å®Œæˆ (å« Target Scalers)")

    # 3. ä¸‹è¼‰æ•¸æ“š & è¨ˆç®—ç‰¹å¾µ
    # [ä¿®æ­£] yfinance çš„ end åƒæ•¸æ˜¯ exclusiveï¼Œéœ€è¦åŠ ä¸€å¤©æ‰èƒ½åŒ…å«ç•¶æ—¥
    end_dt = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1)
    download_end = end_dt.strftime('%Y-%m-%d')
    print(f"\n[Compute] ä¸‹è¼‰æ•¸æ“š (2020-01-01 ~ {end_date}, yf.end={download_end})...")
    raw_df = yf.download("^TWII", start="2020-01-01", end=download_end, auto_adjust=True, progress=False)
    
    # ç¢ºä¿ columns æ ¼å¼æ­£ç¢º
    if isinstance(raw_df.columns, pd.MultiIndex):
        raw_df.columns = raw_df.columns.get_level_values(0)
    
    # [ä¿®æ­£] å–å¾—å¯¦éš›ä¸‹è¼‰è³‡æ–™çš„æœ€å¾Œæ—¥æœŸ (é¿å…é€±æœ«/ç›¤ä¸­åŸ·è¡Œæ™‚æ—¥æœŸä¸ç¬¦)
    actual_last_date = raw_df.index[-1].strftime('%Y-%m-%d')
    print(f"[Data] å¯¦éš›è³‡æ–™æœ€å¾Œæ—¥æœŸ: {actual_last_date}")
    
    print(f"[Compute] è¨ˆç®—ç‰¹å¾µä¸­ (ä½¿ç”¨ç•¶æ—¥æ¨¡å‹)...")
    # å¼·åˆ¶ä¸ä½¿ç”¨å¿«å–ï¼Œç¢ºä¿é‡æ–°è¨ˆç®—
    df = core_system.calculate_features(raw_df, raw_df, ticker="^TWII", use_cache=False)
    
    # å­˜å…¥ç•¶æ—¥å¿«å–
    cache_file = os.path.join(workspace['cache'], 'twii_features.pkl')
    with open(cache_file, 'wb') as f:
        pickle.dump(df, f)
    print(f"[Cache] ç‰¹å¾µå·²å­˜æª”: {cache_file}")
    
    return df, actual_last_date  # [ä¿®æ­£] å›å‚³å¯¦éš›æ—¥æœŸä¾›å ±å‘Šä½¿ç”¨


# =============================================================================
# Step 3: é›™æ¨¡å‹æ¨è«– (ä¿®æ­£ç‰ˆ - è§£æ±º CUDA Tensor Error)
# =============================================================================
def dual_inference(workspace: dict, df: pd.DataFrame) -> dict:
    print("\n" + "=" * 60)
    print("ğŸ¯ Step 3: é›™æ¨¡å‹æ¨è«–")
    print("=" * 60)
    
    from stable_baselines3 import PPO
    
    # æº–å‚™ç‰¹å¾µ
    FEATURE_COLS = core_system.FEATURE_COLS
    latest = df.iloc[-1]
    
    # ç¢ºä¿ç‰¹å¾µæ¬„ä½å°é½Š
    features = []
    for col in FEATURE_COLS:
        val = latest.get(col, 0.0)
        features.append(val)
    features = np.array(features, dtype=np.float32).reshape(1, -1)
    
    # è™•ç† NaN/Inf
    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
    
    results = {}
    
    def run_strategy(name, path, key):
        buy_path = os.path.join(path, 'ppo_buy_twii_final.zip')
        sell_path = os.path.join(path, 'ppo_sell_twii_final.zip')
        
        if not os.path.exists(buy_path):
            results[key] = {'error': 'Model not found'}
            print(f"  [Warning] {name}: æ¨¡å‹ä¸å­˜åœ¨")
            return

        try:
            buy_agent = PPO.load(buy_path)
            sell_agent = PPO.load(sell_path)
            
            # Buy Action
            b_act, _ = buy_agent.predict(features, deterministic=True)
            # Buy Probability
            b_obs = buy_agent.policy.obs_to_tensor(features)[0]
            # [ä¿®æ­£] åŠ ä¸Š .cpu() å†è½‰ numpy
            b_prob = buy_agent.policy.get_distribution(b_obs).distribution.probs.detach().cpu().numpy()[0]
            
            # Sell Action (Construct Sell State: Features + [Current_Return=1.0])
            s_feat = np.concatenate([features[0], [1.0]]).reshape(1, -1)
            s_act, _ = sell_agent.predict(s_feat, deterministic=True)
            
            # Sell Probability (Optionally capture probability if needed)
            # s_obs = sell_agent.policy.obs_to_tensor(s_feat)[0]
            # s_prob = sell_agent.policy.get_distribution(s_obs).distribution.probs.detach().cpu().numpy()[0]
            
            results[key] = {
                'name': name,
                'buy_signal': 'BUY' if b_act[0] == 1 else 'WAIT',
                'buy_prob': float(b_prob[1]) if b_act[0] == 1 else float(b_prob[0]),
                'sell_signal': 'SELL' if s_act[0] == 1 else 'HOLD'
            }
            print(f"  [{name}] Buy: {results[key]['buy_signal']} ({results[key]['buy_prob']:.1%}) | Sell: {results[key]['sell_signal']}")
            
        except Exception as e:
            results[key] = {'error': str(e)}
            print(f"  [Error] {name}: {e}")
            import traceback
            traceback.print_exc() # å°å‡ºè©³ç´°éŒ¯èª¤ä»¥ä¾¿é™¤éŒ¯

    # åŸ·è¡Œ A (Aggressive)
    run_strategy("Aggressive (ROI 85%)", STRATEGY_A_PATH, 'A')
    
    # åŸ·è¡Œ B (Conservative)
    run_strategy("Conservative (MDD -6%)", STRATEGY_B_PATH, 'B')
    
    return results


# =============================================================================
# Step 4: è¼¸å‡ºå ±å‘Š
# =============================================================================
def generate_report(workspace: dict, df: pd.DataFrame, res: dict, date_str: str):
    print("\n" + "=" * 60)
    print("ğŸ“Š Step 4: æˆ°æƒ…å„€è¡¨æ¿")
    print("=" * 60)
    
    last = df.iloc[-1]
    
    lines = []
    lines.append(f"ğŸ“… æ—¥æœŸ: {date_str}")
    lines.append(f"ğŸ“Š æ”¶ç›¤: {last['Close']:.2f} | é‡: {last['Volume']/1e8:.2f}å„„")
    lines.append("-" * 40)
    lines.append("ğŸ”® [åˆ†æå¸« LSTM]")
    lines.append(f"   T+1 æ¼²è·Œ: {last.get('LSTM_Pred_1d', 0)*100:+.2f}%")
    lines.append(f"   T+5 æ¼²è·Œ: {last.get('LSTM_Pred_5d', 0)*100:+.2f}%")
    lines.append(f"   ä¿¡å¿ƒåº¦:   {last.get('LSTM_Conf_5d', 0)*100:.1f}%")
    lines.append("-" * 40)
    lines.append("ğŸ¤– [æ“ç›¤æ‰‹ RL]")
    
    if 'A' in res and 'error' not in res['A']:
        r = res['A']
        icon = "ğŸš€" if r['buy_signal'] == 'BUY' else "ğŸ’¤"
        lines.append(f"   {icon} ç­–ç•¥ A (ç©æ¥µ): [{r['buy_signal']}] (æ©Ÿç‡ {r['buy_prob']:.1%})")
    
    if 'B' in res and 'error' not in res['B']:
        r = res['B']
        icon = "ğŸ›¡ï¸" if r['buy_signal'] == 'BUY' else "ğŸ’¤"
        lines.append(f"   {icon} ç­–ç•¥ B (ä¿å®ˆ): [{r['buy_signal']}] (æ©Ÿç‡ {r['buy_prob']:.1%})")
        
    # ç¶œåˆå»ºè­°
    lines.append("-" * 40)
    sig_a = res.get('A', {}).get('buy_signal', 'N/A')
    sig_b = res.get('B', {}).get('buy_signal', 'N/A')
    
    if sig_a == 'BUY' and sig_b == 'BUY':
        advice = "â­â­ å¼·åŠ›è²·é€² (Strong Buy) â­â­"
    elif sig_a == 'WAIT' and sig_b == 'WAIT':
        advice = "ğŸ’¤ ç©ºæ‰‹è§€æœ› (Wait)"
    elif sig_a == 'BUY':
        advice = "âš ï¸ åƒ…ç©æ¥µå‹è²·é€² (Aggressive Only)"
    else:
        advice = "â“ è¨Šè™Ÿä¸æ˜"
        
    lines.append(f"ğŸ’¡ ç¶œåˆå»ºè­°: {advice}")
    lines.append("=" * 60)
    
    report = "\n".join(lines)
    print(report)
    
    # å­˜æª” TXT
    txt_path = os.path.join(workspace['reports'], 'summary.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # å­˜æª” JSON (æ–¹ä¾¿è‡ªå‹•åŒ–è®€å–)
    json_path = os.path.join(workspace['reports'], 'summary.json')
    json_data = {
        'date': date_str,
        'generated_at': datetime.now().isoformat(),
        'market': {
            'close': float(last.get('Close', 0)),
            'volume': float(last.get('Volume', 0)),
        },
        'lstm': {
            'pred_1d': float(last.get('LSTM_Pred_1d', 0)),
            'pred_5d': float(last.get('LSTM_Pred_5d', 0)),
            'conf_5d': float(last.get('LSTM_Conf_5d', 0)),
        },
        'strategies': res,
        'advice': advice,
    }
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n[Report] å·²å„²å­˜: {txt_path}")
    print(f"[Report] å·²å„²å­˜: {json_path}")


# =============================================================================
# Main
# =============================================================================
def main():
    today = datetime.now()
    # è™•ç†é€±æœ« (å¾€å‰æ¨åˆ°é€±äº”) - ç”¨æ–¼åˆæ­¥ä¼°è¨ˆæ—¥æœŸ
    if today.weekday() == 5: today -= timedelta(days=1)
    elif today.weekday() == 6: today -= timedelta(days=2)
    
    date_str = today.strftime('%Y-%m-%d')
    print(f"ğŸš€ å•Ÿå‹•æ¯æ—¥ç¶­é‹ç³»çµ± - {date_str}")
    
    # Step 0
    ws = create_daily_workspace(date_str)
    
    # Step 1 (Train up to Today)
    train_and_archive_lstm(ws, date_str)
    
    # Step 2 - [ä¿®æ­£] æ¥æ”¶å¯¦éš›è³‡æ–™æ—¥æœŸ
    df, actual_date = isolated_feature_engineering(ws, date_str)
    
    # [ä¿®æ­£] å¦‚æœå¯¦éš›æ—¥æœŸèˆ‡é ä¼°æ—¥æœŸä¸åŒï¼Œé¡¯ç¤ºè­¦å‘Š
    if actual_date != date_str:
        print(f"[Warning] é ä¼°æ—¥æœŸ {date_str} èˆ‡å¯¦éš›è³‡æ–™æ—¥æœŸ {actual_date} ä¸åŒ")
        print(f"[Info] å ±å‘Šå°‡ä½¿ç”¨å¯¦éš›è³‡æ–™æ—¥æœŸ: {actual_date}")
    
    # Step 3
    res = dual_inference(ws, df)
    
    # Step 4 - [ä¿®æ­£] ä½¿ç”¨å¯¦éš›è³‡æ–™æ—¥æœŸç”Ÿæˆå ±å‘Š
    generate_report(ws, df, res, actual_date)

if __name__ == "__main__":
    main()