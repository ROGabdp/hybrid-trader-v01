# -*- coding: utf-8 -*-
"""
================================================================================
Daily Operations V4 - å–®ä¸€ç­–ç•¥æ¯æ—¥ç¶­é‹è…³æœ¬ (è¼•é‡åŒ–å¾®èª¿ç‰ˆæœ¬)
================================================================================
å°ˆç‚º V4 æ¨¡å‹è¨­è¨ˆçš„æ¯æ—¥ç¶­é‹è…³æœ¬ (è¼•é‡åŒ– Fine-tune)

åŠŸèƒ½ï¼š
1. LSTM æ¯æ—¥é‡è¨“èˆ‡å°å­˜ (å‹•æ…‹å¤©æ•¸ + split_ratio 0.99)
2. éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)
3. V4 å–®ä¸€ç­–ç•¥æ¨è«–
4. ç°¡åŒ–å ±å‘Šè¼¸å‡º

V4 ç‰¹é»ï¼š
- Buy Fine-tune: 200K æ­¥ (ä¿ç•™æ›´å¤šé è¨“ç·´çŸ¥è­˜)
- Sell Fine-tune: 100K æ­¥

ä½œè€…ï¼šPhil Liang (Generated by Gemini)
æ—¥æœŸï¼š2025-12-07
================================================================================
"""

import os
import sys
import shutil
import pickle
import subprocess
import json
import glob
from datetime import datetime, timedelta

# è¨­å®š UTF-8 è¼¸å‡º
sys.stdout.reconfigure(encoding='utf-8')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import yfinance as yf
from tensorflow import keras
from keras import layers

# =============================================================================
# å¼•ç”¨ä¸»ç³»çµ±
# =============================================================================
import ptrl_hybrid_system as core_system

# =============================================================================
# è¨­å®šè·¯å¾‘
# =============================================================================
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
DAILY_RUNS_PATH = os.path.join(PROJECT_PATH, 'daily_runs_v4')  # V4 å°ˆç”¨è¼¸å‡ºç›®éŒ„

# V4 æ¨¡å‹è·¯å¾‘ (å–®ä¸€ç­–ç•¥)
V4_MODEL_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v4')

# LSTM è¨“ç·´è…³æœ¬åç¨±
SCRIPT_5D = "twii_model_registry_5d.py"
SCRIPT_1D = "twii_model_registry_multivariate.py"

# LSTM æ¨¡å‹é è¨­è¼¸å‡ºè·¯å¾‘
DEFAULT_LSTM_5D_DIR = os.path.join(PROJECT_PATH, 'saved_models_5d')
DEFAULT_LSTM_1D_DIR = os.path.join(PROJECT_PATH, 'saved_models_multivariate')


# =============================================================================
# Step 0: å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€
# =============================================================================
def create_daily_workspace(date_str: str) -> dict:
    daily_path = os.path.join(DAILY_RUNS_PATH, date_str)
    paths = {
        'root': daily_path,
        'lstm_models': os.path.join(daily_path, 'lstm_models'),
        'lstm_5d': os.path.join(daily_path, 'lstm_models', 'saved_models_5d'),
        'lstm_1d': os.path.join(daily_path, 'lstm_models', 'saved_models_multivariate'),
        'cache': os.path.join(daily_path, 'cache'),
        'reports': os.path.join(daily_path, 'reports'),
    }
    for key, path in paths.items():
        os.makedirs(path, exist_ok=True)
    print(f"[Workspace] å»ºç«‹ç•¶æ—¥å·¥ä½œå€: {daily_path}")
    return paths


# =============================================================================
# Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜ (å‹•æ…‹å¤©æ•¸ + å…¨é‡å­¸ç¿’)
# =============================================================================
def train_and_archive_lstm(workspace: dict, end_date: str):
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜")
    print("=" * 60)
    
    # å‹•æ…‹è¨ˆç®—èµ·å§‹æ—¥æœŸ
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    start_5d = (end_dt - timedelta(days=1800)).strftime('%Y-%m-%d')
    start_1d = (end_dt - timedelta(days=1600)).strftime('%Y-%m-%d')
    
    split_ratio = "0.99"
    
    # 1. åŸ·è¡Œ T+5 è¨“ç·´
    print(f"\n[Training] T+5 Model ({start_5d} ~ {end_date}, split={split_ratio})...")
    script_5d_path = os.path.join(PROJECT_PATH, SCRIPT_5D)
    cmd_5d = [sys.executable, script_5d_path, "train", "--start", start_5d, "--end", end_date, "--split_ratio", split_ratio]
    try:
        subprocess.run(cmd_5d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+5 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+5 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_5d_path}")
        return False
    except Exception as e:
        print(f"[Error] åŸ·è¡ŒéŒ¯èª¤: {e}")
        return False

    # 2. åŸ·è¡Œ T+1 è¨“ç·´
    print(f"\n[Training] T+1 Model ({start_1d} ~ {end_date}, split={split_ratio})...")
    script_1d_path = os.path.join(PROJECT_PATH, SCRIPT_1D)
    cmd_1d = [sys.executable, script_1d_path, "train", "--start", start_1d, "--end", end_date, "--split_ratio", split_ratio]
    try:
        subprocess.run(cmd_1d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+1 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+1 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_1d_path}")
        return False

    # 3. å°å­˜æ¨¡å‹
    print("\n[Archive] å°å­˜æ¨¡å‹åˆ°ç•¶æ—¥å·¥ä½œå€...")
    
    def archive_dir(src_dir, dest_dir):
        if os.path.exists(src_dir):
            if os.path.exists(dest_dir):
                shutil.rmtree(dest_dir)
            shutil.copytree(src_dir, dest_dir)
            print(f"  âœ… å·²å°å­˜: {os.path.basename(src_dir)} -> {dest_dir}")
        else:
            print(f"  âš ï¸ ä¾†æºç›®éŒ„ä¸å­˜åœ¨: {src_dir}")

    archive_dir(DEFAULT_LSTM_5D_DIR, workspace['lstm_5d'])
    archive_dir(DEFAULT_LSTM_1D_DIR, workspace['lstm_1d'])
    
    return True


# =============================================================================
# Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)
# =============================================================================
def isolated_feature_engineering(workspace: dict, end_date: str) -> pd.DataFrame:
    print("\n" + "=" * 60)
    print("ğŸ”§ Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)")
    print("=" * 60)
    
    try:
        from twii_model_registry_5d import SelfAttention
        print("[System] æˆåŠŸå¼•ç”¨åŸå§‹ SelfAttention é¡åˆ¥")
    except ImportError:
        print("[Error] ç„¡æ³•å¼•ç”¨ twii_model_registry_5dï¼Œè«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å­˜åœ¨")
        sys.exit(1)

    def load_model_components(model_dir):
        keras_files = glob.glob(os.path.join(model_dir, "*.keras"))
        if not keras_files: return None, None, None, None
        
        latest_keras = sorted(keras_files)[-1]
        print(f"  ...Loading {os.path.basename(latest_keras)}")
        
        model = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})

        meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
        meta = {}
        if os.path.exists(meta_file):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
            except Exception as e:
                print(f"  âš ï¸ è¼‰å…¥ meta å¤±æ•—: {e}")

        scaler_feat_file = latest_keras.replace('model_', 'feature_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_feat_file):
             scaler_feat_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
        
        scaler_feat = None
        if os.path.exists(scaler_feat_file):
            with open(scaler_feat_file, 'rb') as f:
                scaler_feat = pickle.load(f)

        scaler_tgt_file = latest_keras.replace('model_', 'target_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_tgt_file):
             scaler_tgt = scaler_feat
        else:
             with open(scaler_tgt_file, 'rb') as f:
                 scaler_tgt = pickle.load(f)

        return model, scaler_feat, scaler_tgt, meta

    print("\n[Model Injection] è¼‰å…¥ç•¶æ—¥å°å­˜çš„ LSTM æ¨¡å‹...")
    m5d, sf5d, st5d, meta5d = load_model_components(workspace['lstm_5d'])
    m1d, sf1d, st1d, meta1d = load_model_components(workspace['lstm_1d'])
    
    if m5d is None or m1d is None:
        print("[Error] æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œç‰¹å¾µå·¥ç¨‹")
        sys.exit(1)

    print("\n[Model Injection] æ³¨å…¥ core_system._LSTM_MODELS...")
    if not hasattr(core_system, '_LSTM_MODELS'):
        core_system._LSTM_MODELS = {}
    
    core_system._LSTM_MODELS.update({
        'model_5d': m5d, 'scaler_feat_5d': sf5d, 'scaler_tgt_5d': st5d, 'meta_5d': meta5d,
        'model_1d': m1d, 'scaler_feat_1d': sf1d, 'scaler_tgt_1d': st1d, 'meta_1d': meta1d,
        'loaded': True
    })
    print("  âœ… æ³¨å…¥å®Œæˆ (å« Target Scalers)")

    end_dt = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1)
    download_end = end_dt.strftime('%Y-%m-%d')
    print(f"\n[Compute] ä¸‹è¼‰æ•¸æ“š (2020-01-01 ~ {end_date}, yf.end={download_end})...")
    raw_df = yf.download("^TWII", start="2020-01-01", end=download_end, auto_adjust=True, progress=False)
    
    if isinstance(raw_df.columns, pd.MultiIndex):
        raw_df.columns = raw_df.columns.get_level_values(0)
    
    actual_last_date = raw_df.index[-1].strftime('%Y-%m-%d')
    print(f"[Data] å¯¦éš›è³‡æ–™æœ€å¾Œæ—¥æœŸ: {actual_last_date}")
    
    print(f"[Compute] è¨ˆç®—ç‰¹å¾µä¸­ (ä½¿ç”¨ç•¶æ—¥æ¨¡å‹ + 30æ¬¡ MC Dropout)...")
    df = core_system.calculate_features(raw_df, raw_df, ticker="^TWII", use_cache=False)
    
    cache_file = os.path.join(workspace['cache'], 'twii_features.pkl')
    with open(cache_file, 'wb') as f:
        pickle.dump(df, f)
    print(f"[Cache] ç‰¹å¾µå·²å­˜æª”: {cache_file}")
    
    return df, actual_last_date


# =============================================================================
# Step 3: V4 å–®ä¸€ç­–ç•¥æ¨è«–
# =============================================================================
def single_strategy_inference(workspace: dict, df: pd.DataFrame) -> dict:
    print("\n" + "=" * 60)
    print("ğŸ¯ Step 3: V4 ç­–ç•¥æ¨è«– (è¼•é‡åŒ–)")
    print("=" * 60)
    
    from stable_baselines3 import PPO
    
    FEATURE_COLS = core_system.FEATURE_COLS
    latest = df.iloc[-1]
    
    features = []
    for col in FEATURE_COLS:
        val = latest.get(col, 0.0)
        features.append(val)
    features = np.array(features, dtype=np.float32).reshape(1, -1)
    
    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
    
    result = {}
    
    buy_path = os.path.join(V4_MODEL_PATH, 'ppo_buy_twii_final.zip')
    sell_path = os.path.join(V4_MODEL_PATH, 'ppo_sell_twii_final.zip')
    
    if not os.path.exists(buy_path):
        print(f"[Error] V4 æ¨¡å‹ä¸å­˜åœ¨: {buy_path}")
        return {'error': 'Model not found'}

    try:
        buy_agent = PPO.load(buy_path)
        sell_agent = PPO.load(sell_path)
        
        b_act, _ = buy_agent.predict(features, deterministic=True)
        b_obs = buy_agent.policy.obs_to_tensor(features)[0]
        b_prob = buy_agent.policy.get_distribution(b_obs).distribution.probs.detach().cpu().numpy()[0]
        
        s_feat = np.concatenate([features[0], [1.0]]).reshape(1, -1)
        s_act, _ = sell_agent.predict(s_feat, deterministic=True)
        
        result = {
            'buy_signal': 'BUY' if b_act[0] == 1 else 'WAIT',
            'buy_prob': float(b_prob[1]) if b_act[0] == 1 else float(b_prob[0]),
            'sell_signal': 'SELL' if s_act[0] == 1 else 'HOLD'
        }
        
        buy_icon = "ğŸš€" if result['buy_signal'] == 'BUY' else "ğŸ’¤"
        sell_icon = "ğŸ“¤" if result['sell_signal'] == 'SELL' else "ğŸ›¡ï¸"
        print(f"  {buy_icon} Buy:  {result['buy_signal']} ({result['buy_prob']:.1%})")
        print(f"  {sell_icon} Sell: {result['sell_signal']}")
        
    except Exception as e:
        result = {'error': str(e)}
        print(f"  [Error] æ¨è«–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

    return result


# =============================================================================
# Step 4: è¼¸å‡ºå ±å‘Š
# =============================================================================
def generate_report(workspace: dict, df: pd.DataFrame, res: dict, date_str: str):
    print("\n" + "=" * 60)
    print("ğŸ“Š Step 4: æˆ°æƒ…å„€è¡¨æ¿ (V4)")
    print("=" * 60)
    
    last = df.iloc[-1]
    
    lines = []
    lines.append("=" * 50)
    lines.append(f"ğŸ“… æ—¥æœŸ: {date_str}")
    lines.append("=" * 50)
    
    lines.append("\nğŸ“ˆ [å¸‚å ´æ•¸æ“š]")
    lines.append(f"   æ”¶ç›¤åƒ¹: {last['Close']:.2f}")
    lines.append(f"   æˆäº¤é‡: {last['Volume']/1e8:.2f} å„„")
    
    lines.append("\nğŸ”® [åˆ†æå¸« LSTM]")
    pred_1d = last.get('LSTM_Pred_1d', 0) * 100
    pred_5d = last.get('LSTM_Pred_5d', 0) * 100
    conf_5d = last.get('LSTM_Conf_5d', 0) * 100
    
    lines.append(f"   T+1 é æ¸¬æ¼²è·Œ: {pred_1d:+.2f}%")
    lines.append(f"   T+5 é æ¸¬æ¼²è·Œ: {pred_5d:+.2f}%")
    lines.append(f"   ä¿¡å¿ƒåº¦:       {conf_5d:.1f}%")
    
    lines.append("\nğŸ¤– [æ“ç›¤æ‰‹ V4] (è¼•é‡åŒ–)")
    if 'error' not in res:
        buy_icon = "ğŸš€" if res['buy_signal'] == 'BUY' else "ğŸ’¤"
        sell_icon = "ğŸ“¤" if res['sell_signal'] == 'SELL' else "ğŸ›¡ï¸"
        lines.append(f"   {buy_icon} è²·é€²è¨Šè™Ÿ: {res['buy_signal']} ({res['buy_prob']:.1%})")
        lines.append(f"   {sell_icon} è³£å‡ºè¨Šè™Ÿ: {res['sell_signal']}")
        
        if res['buy_signal'] == 'BUY':
            if conf_5d >= 80:
                advice = "â­â­ é«˜ä¿¡å¿ƒè²·é€² (High Confidence Buy)"
            else:
                advice = "âš ï¸ ä½ä¿¡å¿ƒè²·é€² (Low Confidence Buy)"
        elif res['sell_signal'] == 'SELL':
            advice = "ğŸ“¤ å»ºè­°è³£å‡º (Sell)"
        else:
            advice = "ğŸ’¤ ç©ºæ‰‹è§€æœ› (Wait)"
    else:
        lines.append(f"   âŒ éŒ¯èª¤: {res['error']}")
        advice = "â“ æ¨è«–å¤±æ•—"
    
    lines.append("\n" + "-" * 50)
    lines.append(f"ğŸ’¡ ç¶œåˆå»ºè­°: {advice}")
    lines.append("=" * 50)
    
    report = "\n".join(lines)
    print(report)
    
    txt_path = os.path.join(workspace['reports'], 'summary.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    json_path = os.path.join(workspace['reports'], 'summary.json')
    json_data = {
        'date': date_str,
        'generated_at': datetime.now().isoformat(),
        'version': 'V4',
        'market': {
            'close': float(last.get('Close', 0)),
            'volume': float(last.get('Volume', 0)),
        },
        'lstm': {
            'pred_1d': float(last.get('LSTM_Pred_1d', 0)),
            'pred_5d': float(last.get('LSTM_Pred_5d', 0)),
            'conf_5d': float(last.get('LSTM_Conf_5d', 0)),
        },
        'v4_strategy': res,
        'advice': advice,
    }
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n[Report] å·²å„²å­˜: {txt_path}")
    print(f"[Report] å·²å„²å­˜: {json_path}")


# =============================================================================
# Main
# =============================================================================
def main():
    today = datetime.now()
    if today.weekday() == 5: today -= timedelta(days=1)
    elif today.weekday() == 6: today -= timedelta(days=2)
    
    date_str = today.strftime('%Y-%m-%d')
    print("=" * 60)
    print(f"ğŸš€ V4 æ¯æ—¥ç¶­é‹ç³»çµ± (è¼•é‡åŒ–) - {date_str}")
    print("=" * 60)
    
    if not os.path.exists(os.path.join(V4_MODEL_PATH, 'ppo_buy_twii_final.zip')):
        print(f"\n[Error] V4 æ¨¡å‹ä¸å­˜åœ¨: {V4_MODEL_PATH}")
        print("è«‹å…ˆåŸ·è¡Œ train_v4_models.py å®Œæˆè¨“ç·´")
        sys.exit(1)
    
    ws = create_daily_workspace(date_str)
    
    train_and_archive_lstm(ws, date_str)
    
    df, actual_date = isolated_feature_engineering(ws, date_str)
    
    if actual_date != date_str:
        print(f"[Warning] é ä¼°æ—¥æœŸ {date_str} èˆ‡å¯¦éš›è³‡æ–™æ—¥æœŸ {actual_date} ä¸åŒ")
        print(f"[Info] å ±å‘Šå°‡ä½¿ç”¨å¯¦éš›è³‡æ–™æ—¥æœŸ: {actual_date}")
    
    res = single_strategy_inference(ws, df)
    
    generate_report(ws, df, res, actual_date)
    
    print("\n" + "=" * 60)
    print("âœ… V4 æ¯æ—¥ç¶­é‹å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
